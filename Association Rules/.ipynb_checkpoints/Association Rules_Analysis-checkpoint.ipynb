{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "406a7950",
   "metadata": {},
   "source": [
    "# Association Rule Mining with One-Column Dataset\n",
    "This notebook demonstrates how to apply association rule mining using the Apriori algorithm on a dataset containing a single column of words. Each row represents a transaction, and the words within it represent items purchased together.\n",
    "\n",
    "### Objectives:\n",
    "- Preprocess the dataset\n",
    "- Apply the Apriori algorithm\n",
    "- Extract meaningful association rules\n",
    "- Visualize and interpret the results\n",
    "- Answer common interview questions related to association rule mining\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efb5359",
   "metadata": {},
   "source": [
    "## Task 1: Data Preprocessing\n",
    "We will clean the dataset by splitting the words into individual items and converting the data into the appropriate format for rule mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99e669b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[shrimp, almonds, avocado, vegetables mix, gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[burgers, meatballs, eggs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[chutney]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[turkey, avocado]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[mineral water, milk, energy bar, whole wheat ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               words\n",
       "0  [shrimp, almonds, avocado, vegetables mix, gre...\n",
       "1                         [burgers, meatballs, eggs]\n",
       "2                                          [chutney]\n",
       "3                                  [turkey, avocado]\n",
       "4  [mineral water, milk, energy bar, whole wheat ..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the one-column dataset\n",
    "file_path = 'Online retail.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Split the items in the 'words' column by commas to create a list of items for each transaction\n",
    "data['words'] = data['words'].apply(lambda x: x.split(','))\n",
    "\n",
    "# Display the first few rows of the preprocessed dataset\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b50abc",
   "metadata": {},
   "source": [
    "## Task 2: Convert Data for Apriori Algorithm\n",
    "To apply the Apriori algorithm, we need to convert the data into a format where each item in each transaction is represented as 1 (purchased) or 0 (not purchased)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2ee85ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlxtend'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Convert the dataset into the format required for the Apriori algorithm\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlxtend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransactionEncoder\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create a list of transactions (each row is a list of items)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m transactions \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mlxtend'"
     ]
    }
   ],
   "source": [
    "# Convert the dataset into the format required for the Apriori algorithm\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Create a list of transactions (each row is a list of items)\n",
    "transactions = data['words'].tolist()\n",
    "\n",
    "# Apply the TransactionEncoder to one-hot encode the transactions\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "basket_sets = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Display the first few rows of the transformed dataset\n",
    "basket_sets.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d98feb4",
   "metadata": {},
   "source": [
    "## Task 3: Association Rule Mining with Apriori Algorithm\n",
    "We will now apply the Apriori algorithm to find frequent itemsets and generate association rules. We'll set thresholds for support, confidence, and lift to extract meaningful rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3854333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply the Apriori algorithm to find frequent itemsets\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Set a minimum support threshold (e.g., 0.01 for 1%)\n",
    "frequent_itemsets = apriori(basket_sets, min_support=0.01, use_colnames=True)\n",
    "\n",
    "# Generate association rules with a minimum confidence threshold of 0.2\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.2)\n",
    "\n",
    "# Sort the rules by lift and display the top 10\n",
    "rules = rules.sort_values('lift', ascending=False)\n",
    "rules.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eb8303",
   "metadata": {},
   "source": [
    "## Task 4: Visualization of Association Rules\n",
    "We will visualize the top association rules based on their lift value to understand the strongest product associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa10616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize the top 10 association rules based on lift\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=rules['lift'].head(10), y=rules['consequents'].head(10).astype(str))\n",
    "plt.title('Top 10 Association Rules by Lift')\n",
    "plt.xlabel('Lift')\n",
    "plt.ylabel('Consequent Items')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ffcd54",
   "metadata": {},
   "source": [
    "## Task 5: Interview Questions and Answers\n",
    "Here are some common interview questions related to the Apriori algorithm and association rule mining, with detailed answers.\n",
    "\n",
    "**1. What is Lift and why is it important in Association Rules?**\n",
    "**Answer**: Lift is a measure of how much more likely two items are to be purchased together compared to if they were independent. A lift greater than 1 indicates a strong association, meaning that the occurrence of the antecedent increases the likelihood of the consequent occurring. Lift helps in identifying the strength of the relationship between items.\n",
    "\n",
    "**2. What is Support and Confidence? How do you calculate them?**\n",
    "**Answer**: Support is the proportion of transactions that contain a particular itemset. It indicates how frequently an item or set of items appears in the dataset. Confidence is a measure of the likelihood that the consequent will occur given that the antecedent has occurred. Both metrics help in determining which rules are meaningful.\n",
    "\n",
    "- **Support**: Support(X) = (Transactions containing X) / (Total Transactions)\n",
    "- **Confidence**: Confidence(X -> Y) = Support(X and Y) / Support(X)\n",
    "\n",
    "**3. What are some limitations or challenges of Association Rules mining?**\n",
    "**Answer**: Some challenges include:\n",
    "- Handling large datasets: Frequent itemset generation can be computationally expensive.\n",
    "- Selecting appropriate thresholds: It can be difficult to set meaningful thresholds for support, confidence, and lift.\n",
    "- Overfitting: Too many rules can lead to overfitting, generating rules that are not practically useful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84328225",
   "metadata": {},
   "source": [
    "## Task 6: Conclusion\n",
    "In this notebook, we applied the Apriori algorithm to discover associations between products purchased together in the dataset. The top rules based on lift revealed important product associations, which could be used for targeted marketing or cross-selling strategies.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Support**: Frequency of occurrence of an itemset in transactions.\n",
    "- **Confidence**: Likelihood of the consequent occurring given that the antecedent occurs.\n",
    "- **Lift**: Measure of how much more likely the antecedent and consequent are to occur together compared to if they were independent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
