{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8671e372",
   "metadata": {},
   "source": [
    "# Glass Classification using Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23946b0f",
   "metadata": {},
   "source": [
    "\n",
    "### Objectives:\n",
    "1. Explore the Glass dataset.\n",
    "2. Train and evaluate a Random Forest model to classify glass types.\n",
    "3. Apply Bagging and Boosting techniques.\n",
    "4. Handle potential data imbalance issues.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d15e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('/mnt/data/glass.csv')\n",
    "\n",
    "# Display first few rows of the dataset\n",
    "data.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b213c07",
   "metadata": {},
   "source": [
    "### Data Overview and Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6079469",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display basic info and first few rows of the dataset\n",
    "data.info()\n",
    "data.describe()\n",
    "\n",
    "# Check for missing values\n",
    "data.isnull().sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9016843d",
   "metadata": {},
   "source": [
    "\n",
    "### Exploratory Data Analysis (EDA)\n",
    "We will visualize the distributions of numeric features, check for outliers, and analyze correlations between features.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2674fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Plot histograms for numerical columns\n",
    "data.hist(bins=15, figsize=(12, 10))\n",
    "plt.suptitle('Distribution of Numeric Features')\n",
    "plt.show()\n",
    "\n",
    "# 2. Boxplot for 'RI', 'Na', 'Mg', 'AI', and other features (outlier detection)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x=data['RI'])\n",
    "plt.title('Boxplot of Refractive Index (RI)')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x=data['Na'])\n",
    "plt.title('Boxplot of Sodium (Na)')\n",
    "plt.show()\n",
    "\n",
    "# 3. Correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = data.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490f602d",
   "metadata": {},
   "source": [
    "\n",
    "### Data Preprocessing and Feature Engineering:\n",
    "1. Handle any missing values in the dataset.\n",
    "2. Apply feature scaling using standardization.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711a43ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature Scaling (Standardization)\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(data.drop('Type', axis=1))\n",
    "\n",
    "# Create a new DataFrame for the scaled features\n",
    "scaled_data = pd.DataFrame(scaled_features, columns=data.columns[:-1])\n",
    "scaled_data['Type'] = data['Type']\n",
    "scaled_data.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb348a69",
   "metadata": {},
   "source": [
    "\n",
    "### Random Forest Classifier:\n",
    "We will split the data into training and testing sets, train a Random Forest classifier, and evaluate its performance.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bf497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting data into features (X) and target (y)\n",
    "X = scaled_data.drop('Type', axis=1)\n",
    "y = scaled_data['Type']\n",
    "\n",
    "# Splitting into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Output the performance metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eca4f7",
   "metadata": {},
   "source": [
    "\n",
    "### Bagging and Boosting Methods:\n",
    "We will apply Bagging and Boosting techniques to further improve model performance and compare the results with the Random Forest classifier.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daed7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\n",
    "\n",
    "# Bagging Classifier\n",
    "bagging_clf = BaggingClassifier(base_estimator=rf_clf, n_estimators=50, random_state=42)\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "y_bag_pred = bagging_clf.predict(X_test)\n",
    "bagging_accuracy = accuracy_score(y_test, y_bag_pred)\n",
    "print(f\"Bagging Accuracy: {bagging_accuracy}\")\n",
    "\n",
    "# Boosting Classifier (AdaBoost)\n",
    "boosting_clf = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "boosting_clf.fit(X_train, y_train)\n",
    "y_boost_pred = boosting_clf.predict(X_test)\n",
    "boosting_accuracy = accuracy_score(y_test, y_boost_pred)\n",
    "print(f\"Boosting Accuracy: {boosting_accuracy}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50887dd7",
   "metadata": {},
   "source": [
    "\n",
    "### Handling Imbalanced Data:\n",
    "In case the dataset is imbalanced (i.e., the target classes are not equally represented), methods like oversampling, undersampling, or using class weights in the model can be applied to balance the data.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fdb3bf",
   "metadata": {},
   "source": [
    "\n",
    "## Interview Questions:\n",
    "1. **What is the difference between Bagging and Boosting?**\n",
    "   - **Bagging**: Combines multiple models (usually of the same type) to reduce variance. Each model is trained on a random subset of the data, and their outputs are averaged.\n",
    "   - **Boosting**: Combines multiple models sequentially, where each subsequent model focuses on the errors of the previous models. It reduces both variance and bias.\n",
    "\n",
    "2. **How to handle imbalanced data?**\n",
    "   - **Oversampling/Undersampling**: Increasing or reducing the number of samples in the minority or majority class.\n",
    "   - **Class Weights**: Assigning higher weights to the minority class to penalize misclassifications more heavily.\n",
    "   - **Synthetic Data Generation (SMOTE)**: Creating synthetic samples for the minority class.\n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
