{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e533fe4",
   "metadata": {},
   "source": [
    "# Glass Classification using Random Forest Classifier with Enhanced Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa7b5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('/mnt/data/glass.csv')\n",
    "\n",
    "# Display first few rows of the dataset\n",
    "data.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb728eb6",
   "metadata": {},
   "source": [
    "\n",
    "### Handling Missing Values:\n",
    "We will check and handle any missing values in the dataset, particularly in the target column `Type`.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dc6cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Checking for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Drop rows with missing values in the target column 'Type'\n",
    "data = data.dropna(subset=['Type'])\n",
    "\n",
    "# Confirm no more missing values in the target column\n",
    "print(\"Missing values after handling:\")\n",
    "print(data.isnull().sum())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8ccbb2",
   "metadata": {},
   "source": [
    "\n",
    "### Exploratory Data Analysis (EDA) with Enhanced Visualizations:\n",
    "We will explore the distributions of numerical variables, visualize relationships between features, and apply scaling techniques.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aca15ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Pairplot to visualize relationships between numerical variables\n",
    "sns.pairplot(data, hue='Type', diag_kind='kde')\n",
    "plt.suptitle('Pairplot of Numerical Features with Type of Glass', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# 2. KDE plots for continuous variables (RI, Na, Mg, etc.)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(data['RI'], shade=True, label='RI')\n",
    "sns.kdeplot(data['Na'], shade=True, label='Na')\n",
    "sns.kdeplot(data['Mg'], shade=True, label='Mg')\n",
    "plt.title('KDE Plot of Continuous Features')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 3. Bar chart for Type distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Type', data=data)\n",
    "plt.title('Distribution of Glass Types')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dba617",
   "metadata": {},
   "source": [
    "\n",
    "### Correlation Matrix and Outlier Detection:\n",
    "We will analyze correlations between features and detect any potential outliers using boxplots.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e03d1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = data.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot for 'RI', 'Na', 'Mg', and others to detect outliers\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x=data['RI'])\n",
    "plt.title('Boxplot of Refractive Index (RI)')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x=data['Na'])\n",
    "plt.title('Boxplot of Sodium (Na)')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b49deaf",
   "metadata": {},
   "source": [
    "\n",
    "### Feature Scaling:\n",
    "We will apply standardization to ensure that all features are on the same scale before training the model.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a0e38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature Scaling (Standardization)\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(data.drop('Type', axis=1))\n",
    "\n",
    "# Create a new DataFrame for the scaled features\n",
    "scaled_data = pd.DataFrame(scaled_features, columns=data.columns[:-1])\n",
    "scaled_data['Type'] = data['Type']\n",
    "scaled_data.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8351c34a",
   "metadata": {},
   "source": [
    "\n",
    "### Random Forest Classifier:\n",
    "We will split the cleaned data into training and testing sets, train a Random Forest classifier, and evaluate its performance.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f86572",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting data into features (X) and target (y)\n",
    "X = scaled_data.drop('Type', axis=1)\n",
    "y = scaled_data['Type']\n",
    "\n",
    "# Splitting into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Output the performance metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a120a36",
   "metadata": {},
   "source": [
    "\n",
    "### Feature Importance:\n",
    "We will visualize the importance of features in the Random Forest model to understand which factors contribute most to the classification.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408b7058",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature importance\n",
    "importances = rf_clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=importances[indices], y=X.columns[indices])\n",
    "plt.title('Feature Importance from Random Forest Classifier')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9abadee",
   "metadata": {},
   "source": [
    "\n",
    "### Bagging and Boosting Methods:\n",
    "We will apply Bagging and Boosting techniques to improve model performance and compare the results with the original Random Forest classifier.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19e7c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\n",
    "\n",
    "# Bagging Classifier\n",
    "bagging_clf = BaggingClassifier(base_estimator=rf_clf, n_estimators=50, random_state=42)\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "y_bag_pred = bagging_clf.predict(X_test)\n",
    "bagging_accuracy = accuracy_score(y_test, y_bag_pred)\n",
    "print(f\"Bagging Accuracy: {bagging_accuracy}\")\n",
    "\n",
    "# Boosting Classifier (AdaBoost)\n",
    "boosting_clf = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "boosting_clf.fit(X_train, y_train)\n",
    "y_boost_pred = boosting_clf.predict(X_test)\n",
    "boosting_accuracy = accuracy_score(y_test, y_boost_pred)\n",
    "print(f\"Boosting Accuracy: {boosting_accuracy}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
