{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85ad5216",
   "metadata": {},
   "source": [
    "# Alphabet Classification using Artificial Neural Networks\n",
    "In this notebook, we will build a classification model using Artificial Neural Networks (ANNs) to classify data from the 'Alphabets_data.csv' dataset. We will preprocess the data, implement the ANN model, tune hyperparameters, and evaluate the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb23ac51",
   "metadata": {},
   "source": [
    "## Step 1: Data Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acf7c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Alphabets_data.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1832a546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the dataset\n",
    "print(\"Number of samples:\", data.shape[0])\n",
    "print(\"Number of features:\", data.shape[1])\n",
    "print(\"Class distribution:\")\n",
    "print(data['letter'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f25667",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "- Convert categorical labels to numerical values.\n",
    "- Normalize the feature columns.\n",
    "- Split the dataset into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2937afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data\n",
    "label_encoder = LabelEncoder()\n",
    "data['letter'] = label_encoder.fit_transform(data['letter'])\n",
    "\n",
    "# Separate features and labels\n",
    "X = data.drop(columns=['letter'])\n",
    "y = data['letter']\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953fcdbd",
   "metadata": {},
   "source": [
    "## Step 2: ANN Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461e9a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for ANN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Build a basic ANN model with one hidden layer\n",
    "model = Sequential([\n",
    "    Dense(64, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')  # Multi-class classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a178390",
   "metadata": {},
   "source": [
    "## Step 3: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e63c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Generate classification report\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = model.predict(X_test).argmax(axis=1)\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ff09b0",
   "metadata": {},
   "source": [
    "## Step 4: Hyperparameter Tuning\n",
    "We will tune the following hyperparameters:\n",
    "- Number of hidden layers and neurons\n",
    "- Activation functions\n",
    "- Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb29a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Define the function for creating the ANN model\n",
    "def create_model(activation='relu', optimizer='adam'):\n",
    "    model = Sequential([\n",
    "        Dense(64, input_shape=(X_train.shape[1],), activation=activation),\n",
    "        Dense(32, activation=activation),\n",
    "        Dense(len(label_encoder.classes_), activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap the model for use with GridSearchCV\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'epochs': [10, 20],\n",
    "    'batch_size': [10, 20]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and performance\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41436fd0",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we built an ANN model to classify alphabets from the dataset. We explored the dataset, preprocessed the data, and implemented an ANN model. Finally, we tuned the hyperparameters to improve the model's performance."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
