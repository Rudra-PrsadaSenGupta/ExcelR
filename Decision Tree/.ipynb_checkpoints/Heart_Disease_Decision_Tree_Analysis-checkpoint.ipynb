{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "961513ee",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction using Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198fb600",
   "metadata": {},
   "source": [
    "\n",
    "### Objectives:\n",
    "1. Explore the heart disease dataset.\n",
    "2. Apply Decision Tree Classifier to predict heart disease stages.\n",
    "3. Tune the model for better performance.\n",
    "4. Visualize the decision tree and key features.\n",
    "5. Answer interview questions related to Decision Trees and feature encoding.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8f14f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('/mnt/data/heart_disease.csv')\n",
    "data.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd46fff",
   "metadata": {},
   "source": [
    "### Data Overview and Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be96d5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display basic info and first few rows of the dataset\n",
    "data.info()\n",
    "data.describe()\n",
    "\n",
    "# Check for missing values\n",
    "data.isnull().sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e679102",
   "metadata": {},
   "source": [
    "\n",
    "### Exploratory Data Analysis (EDA)\n",
    "We will visualize the distributions of numeric features, check for outliers, and analyze correlations between features.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8265de02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Plot histograms for numerical columns\n",
    "plt.figure(figsize=(12, 10))\n",
    "data[['age', 'trestbps', 'chol', 'thalch', 'oldpeak']].hist(bins=15, figsize=(12, 10))\n",
    "plt.suptitle('Distribution of Numeric Features')\n",
    "plt.show()\n",
    "\n",
    "# 2. Boxplot for 'trestbps' and 'chol' (outlier detection)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x=data['trestbps'])\n",
    "plt.title('Boxplot of Resting Blood Pressure')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x=data['chol'])\n",
    "plt.title('Boxplot of Cholesterol')\n",
    "plt.show()\n",
    "\n",
    "# 3. Correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = data.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da83faa4",
   "metadata": {},
   "source": [
    "\n",
    "### Feature Engineering:\n",
    "1. Handle missing values in the 'oldpeak' column.\n",
    "2. Encode categorical variables ('sex', 'cp', 'restecg', etc.) for machine learning.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e17145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fill missing values in 'oldpeak' with the median\n",
    "data['oldpeak'] = data['oldpeak'].fillna(data['oldpeak'].median())\n",
    "\n",
    "# Encode categorical columns using LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "categorical_columns = ['sex', 'cp', 'restecg', 'exang', 'slope', 'thal']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    data[col] = encoder.fit_transform(data[col])\n",
    "\n",
    "data.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250adf49",
   "metadata": {},
   "source": [
    "\n",
    "### Decision Tree Classifier:\n",
    "We will split the data into training and testing sets, train a Decision Tree model, and evaluate its performance using accuracy, precision, recall, F1-score, and ROC-AUC.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a093fa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting data into features (X) and target (y)\n",
    "X = data.drop('num', axis=1)\n",
    "y = data['num']\n",
    "\n",
    "# Splitting into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Output the performance metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e5e588",
   "metadata": {},
   "source": [
    "\n",
    "### Hyperparameter Tuning:\n",
    "We will tune the Decision Tree using hyperparameters like 'max_depth', 'min_samples_split', and 'criterion' to optimize the model's performance.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c8cb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Use GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Output best parameters\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c06316",
   "metadata": {},
   "source": [
    "\n",
    "### Decision Tree Visualization:\n",
    "We will visualize the structure of the decision tree and the importance of the features in the model.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9600979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize the Decision Tree\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(clf, feature_names=X.columns, filled=True, rounded=True, class_names=True)\n",
    "plt.show()\n",
    "\n",
    "# Feature importance plot\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=importances[indices], y=X.columns[indices])\n",
    "plt.title('Feature Importance')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f290dac1",
   "metadata": {},
   "source": [
    "\n",
    "## Interview Questions:\n",
    "1. **What are some common hyperparameters of Decision Tree models and how do they affect the model's performance?**\n",
    "   - **max_depth**: Limits the depth of the tree to prevent overfitting.\n",
    "   - **min_samples_split**: Minimum number of samples required to split a node. Increasing this can prevent overfitting by limiting tree growth.\n",
    "   - **criterion**: The function used to measure the quality of a split ('gini' for Gini impurity or 'entropy' for Information Gain). The choice can affect how the tree splits data at each node.\n",
    "\n",
    "2. **What is the difference between Label Encoding and One-Hot Encoding?**\n",
    "   - **Label Encoding**: Assigns a unique integer to each category. This can be useful when categories have an inherent order, but may confuse models when categories are nominal (unordered).\n",
    "   - **One-Hot Encoding**: Creates binary columns for each category, preserving the uniqueness of each category without implying any order.\n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
